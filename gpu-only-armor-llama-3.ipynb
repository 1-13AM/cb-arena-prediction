{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8900034,"sourceType":"datasetVersion","datasetId":5350506},{"sourceId":8915911,"sourceType":"datasetVersion","datasetId":5361778},{"sourceId":8944605,"sourceType":"datasetVersion","datasetId":5381992,"isSourceIdPinned":true}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets bitsandbytes accelerate torch peft wandb -U -qq","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:20:30.973804Z","iopub.execute_input":"2024-07-19T17:20:30.974487Z","iopub.status.idle":"2024-07-19T17:21:59.694226Z","shell.execute_reply.started":"2024-07-19T17:20:30.974449Z","shell.execute_reply":"2024-07-19T17:21:59.693127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n#     torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n#     torch.backends.cudnn.benchmark = False  # Ensure reproducibility, might slow down performance\n    random.seed(seed)\n\n# Set the seed for all libraries\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:21:59.695970Z","iopub.execute_input":"2024-07-19T17:21:59.696252Z","iopub.status.idle":"2024-07-19T17:21:59.704542Z","shell.execute_reply.started":"2024-07-19T17:21:59.696223Z","shell.execute_reply":"2024-07-19T17:21:59.703961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nsrc_paths = [r\"/kaggle/input/armorm-llama3-8b-v0-1-mdl-custom/modeling_custom.py\", r\"/kaggle/input/lmsys-ddp-for-armor-llama/lmsys_ddp.py\"]\ndst_path = r\"/kaggle/working/\"\nfor src_path in src_paths:\n    shutil.copy(src_path, dst_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:21:59.705362Z","iopub.execute_input":"2024-07-19T17:21:59.705578Z","iopub.status.idle":"2024-07-19T17:21:59.734928Z","shell.execute_reply.started":"2024-07-19T17:21:59.705554Z","shell.execute_reply":"2024-07-19T17:21:59.734339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast\nimport re\nfrom typing import List","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T17:21:59.736547Z","iopub.execute_input":"2024-07-19T17:21:59.736835Z","iopub.status.idle":"2024-07-19T17:21:59.740168Z","shell.execute_reply.started":"2024-07-19T17:21:59.736810Z","shell.execute_reply":"2024-07-19T17:21:59.739519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_SIZE = 19\nHIDDEN_SIZE = 128\nNUM_CLASSES = 3\nBATCH_SIZE = 8\nPER_DEVICE_TRAIN_BATCH_SIZE = PER_DEVICE_EVAL_BATCH_SIZE = 1\nNUM_EPOCH = 1\nGRADIENT_ACCUMULATION_STEPS = 8\nMODEL_CKPT = \"RLHFlow/ArmoRM-Llama3-8B-v0.1\"\nLEARNING_RATE = 1e-4\nMAX_LENGTH = 1536\n# Might take a look at LoRA to remind myself of how these hyperparameters work\nLORA_R = 4\nLORA_ALPHA = LORA_R * 2\nLORA_DROPOUT = 0.05\nLORA_BIAS = 'none'","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:21:59.740948Z","iopub.execute_input":"2024-07-19T17:21:59.741181Z","iopub.status.idle":"2024-07-19T17:21:59.751629Z","shell.execute_reply.started":"2024-07-19T17:21:59.741158Z","shell.execute_reply":"2024-07-19T17:21:59.751063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_pickle('/kaggle/input/lmsys-preprocessed-data/preprocessed_data.pkl')\ndef truncate(df, max_length):\n    def truncation_op(x, col: str, max_len: int = 1024):\n        return x[col][:min(len(x[col]), max_len)]\n    \n    df['input_ids_a'] = df.apply(truncation_op, args=('input_ids_a', max_length), axis=1)\n    df['input_ids_b'] = df.apply(truncation_op, args=('input_ids_b', max_length), axis=1)\n    \n    return df\n\ntrain_df = truncate(train_df, MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:21:59.752374Z","iopub.execute_input":"2024-07-19T17:21:59.752580Z","iopub.status.idle":"2024-07-19T17:22:08.907196Z","shell.execute_reply.started":"2024-07-19T17:21:59.752558Z","shell.execute_reply":"2024-07-19T17:22:08.906440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_pattern = [128009, 128006, 78191, 128007, 271]\ndef find_token_for_gating(lst, ):\n    \"\"\"Find the last occurrence of a token_pattern in a list.\"\"\"\n    token_pattern_len = len(token_pattern)\n    search_end = len(lst)\n    for j in range(search_end - token_pattern_len, -1, -1):\n        if lst[j:j + token_pattern_len] == token_pattern:\n            return j\n    return -1","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:08.908323Z","iopub.execute_input":"2024-07-19T17:22:08.908586Z","iopub.status.idle":"2024-07-19T17:22:08.913081Z","shell.execute_reply.started":"2024-07-19T17:22:08.908560Z","shell.execute_reply":"2024-07-19T17:22:08.912468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all data points where the prompt token lengths exceed \nexp = []\nfor i in range(len(train_df)): \n    if(find_token_for_gating(train_df.iloc[i]['input_ids_a']) < 0):\n        exp.append(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:08.913897Z","iopub.execute_input":"2024-07-19T17:22:08.914150Z","iopub.status.idle":"2024-07-19T17:22:14.395388Z","shell.execute_reply.started":"2024-07-19T17:22:08.914126Z","shell.execute_reply":"2024-07-19T17:22:14.394634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove these rows from training data        \ntrain_df = train_df.drop(index=exp)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:14.396509Z","iopub.execute_input":"2024-07-19T17:22:14.396764Z","iopub.status.idle":"2024-07-19T17:22:14.420896Z","shell.execute_reply.started":"2024-07-19T17:22:14.396738Z","shell.execute_reply":"2024-07-19T17:22:14.420116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\nfrom functools import partial\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:14.423269Z","iopub.execute_input":"2024-07-19T17:22:14.423521Z","iopub.status.idle":"2024-07-19T17:22:15.876560Z","shell.execute_reply.started":"2024-07-19T17:22:14.423497Z","shell.execute_reply":"2024-07-19T17:22:15.875523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare datasets","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\ntrain_df_cop = train_df.copy()\ndataset = Dataset.from_pandas(train_df_cop[['input_ids_a', 'input_ids_b', 'label']])\ndataset.set_format(type=\"torch\")\nshuffled_dataset = dataset.shuffle(seed=42)\ntrain_test_split = shuffled_dataset.train_test_split(test_size=0.2)\n\n# Optionally, you can wrap them in a DatasetDict for easier handling\ndataset_dict = DatasetDict({\n    'train': train_test_split['train'],\n    'validation': train_test_split['test']\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:15.877797Z","iopub.execute_input":"2024-07-19T17:22:15.878117Z","iopub.status.idle":"2024-07-19T17:22:21.374369Z","shell.execute_reply.started":"2024-07-19T17:22:15.878086Z","shell.execute_reply":"2024-07-19T17:22:21.373265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a network for model comparison","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modeling_custom import LlamaForRewardModelWithGating\n\nclass SwiGLU(nn.Module):\n    def forward(self, x):\n        x, gate = x.chunk(2, dim=-1)\n        return F.silu(gate) * x\n\nclass DualInputInteractionNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(DualInputInteractionNetwork, self).__init__()\n        self.classification_head = nn.Sequential(nn.Linear(input_size, hidden_size),\n                                                 nn.Linear(hidden_size, hidden_size),\n                                                 nn.Linear(hidden_size, num_classes)  # Combining interaction features\n                                                )\n\n    def forward_one(self, x):\n        x = F.silu(self.shared_fc1(x))\n        x = F.silu(self.shared_fc2(x))\n        return x\n    \n    @staticmethod\n    def SwiGLU(x):\n        x, gate = x.chunk(2, dim=-1)\n        return F.silu(gate) * x\n    \n    def forward(self, inputs: dict, reward_transform_matrix: torch.Tensor):\n        \n        batch_size = inputs.rewards.shape[0]\n        model_a_idx = torch.arange(0, batch_size, 2)\n        model_b_idx = model_a_idx + 1\n        \n        multi_obj_rewards_a = inputs.rewards[model_a_idx]\n        multi_obj_rewards_b = inputs.rewards[model_b_idx]\n        \n        multi_obj_coeffs_a = inputs.gating_output[model_a_idx] @ reward_transform_matrix.T\n        multi_obj_coeffs_b = inputs.gating_output[model_b_idx] @ reward_transform_matrix.T\n        \n        scaled_mul_obj_rewards_a = multi_obj_rewards_a * multi_obj_coeffs_a\n        scaled_mul_obj_rewards_b = multi_obj_rewards_b * multi_obj_coeffs_b\n        \n        output = self.classification_head(scaled_mul_obj_rewards_a - scaled_mul_obj_rewards_b)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:21.375780Z","iopub.execute_input":"2024-07-19T17:22:21.376578Z","iopub.status.idle":"2024-07-19T17:22:21.399249Z","shell.execute_reply.started":"2024-07-19T17:22:21.376537Z","shell.execute_reply":"2024-07-19T17:22:21.398421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Optional\n\nclass LLaMaPreferencePredictionModel(nn.Module):\n    \n    def __init__(self, reward_model_ckpt, reward_model_quant_config: BitsAndBytesConfig, \n                 lora_config: Optional[dict] = None, \n                 **dual_input_interaction_kwargs):\n        \"\"\"\n        there are Gemma variants for ArMor\n        \"\"\"\n        \n        super().__init__()\n        self.reward_model = LlamaForRewardModelWithGating.from_pretrained(reward_model_ckpt, \n                                                                          quantization_config=reward_model_quant_config\n                                                                                            )\n        \n        self.preference_prediction_model = DualInputInteractionNetwork(**dual_input_interaction_kwargs)\n        \n        self.register_buffer('reward_transform_matrix', self.reward_model.reward_transform_matrix)\n        \n        if lora_config is not None:\n            self.lora_setup(**lora_config)\n            \n    def lora_config(self, **lora_config):\n        \n        from peft import LoraConfig, TaskType\n        self.lora_config = LoraConfig(**lora_config,\n                                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n                                task_type=TaskType.SEQ_CLS)\n        \n    def lora_setup(self, **lora_config):\n        \n        from peft import get_peft_model, prepare_model_for_kbit_training\n        self.lora_config(**lora_config)\n        \n        self.reward_model = prepare_model_for_kbit_training(self.reward_model)\n        self.reward_model = get_peft_model(self.reward_model, self.lora_config)\n    \n        \n    def forward(self, input_ids, attention_mask, label=None):\n        # there are data fields that the ArMor model does not require, so passing kwargs would be the temporary solution\n        # armoRM framework only takes input_ids as inputs\n        \n        reward_model_output = self.reward_model(input_ids, attention_mask)\n        logits = self.preference_prediction_model(reward_model_output, self.reward_transform_matrix)\n        if label is None:\n            return {\"reward_model_output\": reward_model_output,\n                    \"logits\": logits}\n        \n        return {\"reward_model_output\": reward_model_output,\n                \"logits\": logits,\n                \"label\": label}","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:53.162076Z","iopub.execute_input":"2024-07-19T17:22:53.162494Z","iopub.status.idle":"2024-07-19T17:22:53.171780Z","shell.execute_reply.started":"2024-07-19T17:22:53.162458Z","shell.execute_reply":"2024-07-19T17:22:53.170995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a custom data collator & trainer for this use case","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\nfrom transformers import DataCollatorWithPadding\nfrom dataclasses import dataclass\nfrom typing import Union, Dict, Any\nfrom transformers.tokenization_utils_base import PaddingStrategy\n\n@dataclass\nclass RewardDataCollatorWithPadding:\n    tokenizer: AutoTokenizer\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        merged_features = []\n        for feature in features:\n            merged_features.append(\n                {\n                    \"input_ids\": feature[\"input_ids_a\"]\n                }\n            )\n            merged_features.append(\n                {\n                    \"input_ids\": feature[\"input_ids_b\"]\n                }\n            )\n        batch = self.tokenizer.pad(\n            merged_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=self.return_tensors,\n        )\n        batch = {\n            \"input_ids\": batch[\"input_ids\"],\n            \"attention_mask\": batch[\"attention_mask\"],\n            \"label\": torch.tensor([feature[\"label\"].item() for feature in features])\n        }\n        \n        return batch\n                    \nclass RewardTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(\n            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], label=inputs['label']\n        )\n        logits, targets = outputs['logits'], outputs['label']\n        loss = nn.functional.cross_entropy(logits, targets).mean()\n        \n        return loss\n\ndef compute_metrics(pred):\n    \n    # Get the predictions and labels from the pred argument\n    logits, labels = pred\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Calculate cross entropy loss\n    # Convert logits to PyTorch tensor\n    logits_tensor = torch.tensor(logits)\n    labels_tensor = torch.tensor(labels)\n    \n    # Compute cross entropy loss\n    cross_entropy_loss = F.cross_entropy(logits_tensor, labels_tensor).item()\n\n    return {\n        \"cross_entropy_loss\": cross_entropy_loss\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:22:54.990837Z","iopub.execute_input":"2024-07-19T17:22:54.991231Z","iopub.status.idle":"2024-07-19T17:22:55.002921Z","shell.execute_reply.started":"2024-07-19T17:22:54.991186Z","shell.execute_reply":"2024-07-19T17:22:55.002169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom modeling_custom import LlamaForRewardModelWithGating\nimport torch\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True, # quantize the quantization factor, which saves another 0.4 bit/parameter\n   bnb_4bit_compute_dtype=torch.float16 # configure computations to be in (b)float16\n)\n\nlora_config = {'r': LORA_R,\n               'lora_alpha': LORA_ALPHA,\n               'lora_dropout': LORA_DROPOUT,\n               'bias': LORA_BIAS\n                }","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:23:00.915306Z","iopub.execute_input":"2024-07-19T17:23:00.916025Z","iopub.status.idle":"2024-07-19T17:23:00.920724Z","shell.execute_reply.started":"2024-07-19T17:23:00.915985Z","shell.execute_reply":"2024-07-19T17:23:00.919990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-GPU training","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset, DistributedSampler\nfrom tqdm import tqdm\nfrom accelerate import Accelerator\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\ndef create_dataloaders(dataset_dict: DatasetDict, data_collator, batch_size: int = 4):\n    train_dataloader = DataLoader(dataset_dict['train'], batch_size=batch_size, collate_fn=data_collator)\n    eval_dataloader = DataLoader(dataset_dict['validation'], batch_size=batch_size, collate_fn=data_collator)\n    return train_dataloader, eval_dataloader\n\ndef train_one_epoch(model, dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch):\n    accelerator.print(\"Training on one epoch\")\n    model.train()\n    epoch_loss, step_loss = 0., 0.\n    for i, batch in enumerate(tqdm(dataloader, desc=f'Epoch {epoch+1}')):\n#         print(batch['input_ids'].shape)\n        with accelerator.accumulate(model):\n            # Forward pass\n            outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch[\"attention_mask\"], \n                label=batch['label']\n            )\n            \n            logits = outputs['logits']\n            loss = criterion(logits, batch['label'])\n            step_loss += loss.item()\n            epoch_loss += loss.item()\n            \n            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n                accelerator.log({\"train_loss\": step_loss / GRADIENT_ACCUMULATION_STEPS, \"epoch\": epoch + 1})\n                step_loss = 0.\n                \n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n    accelerator.print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}\")\n    \ndef eval_one_epoch(model, dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch):\n    model.eval()\n    epoch_loss = 0\n    \n    for batch in tqdm(dataloader):\n\n        # Forward pass\n        outputs = model(\n                input_ids=batch['input_ids'],\n                attention_mask=batch[\"attention_mask\"], \n                label=batch['label']\n            )\n        \n        logits = outputs['logits']\n        loss = criterion(logits, batch['label'])\n        epoch_loss += loss.item()\n    \n    accelerator.log({\"epoch_loss\": epoch_loss / len(dataloader), \"epoch\": epoch + 1})\n    \n    accelerator.print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}\")\n    \n\ndef main_train_loop(model, dataset_dict, data_collator, mixed_precision, batch_size, gradient_accumulation_steps, num_epoch, learning_rate):\n    \n    !pip install peft -U -qq\n    accelerator = Accelerator(mixed_precision=mixed_precision, \n                              gradient_accumulation_steps=gradient_accumulation_steps,\n                              log_with=\"wandb\"\n                             )\n    \n    model = LLaMaPreferencePredictionModel(reward_model_ckpt=MODEL_CKPT, \n                                       reward_model_quant_config=nf4_config, \n                                       lora_config=lora_config,\n                                       input_size=INPUT_SIZE, \n                                       hidden_size=HIDDEN_SIZE, \n                                       num_classes=NUM_CLASSES) \n    \n    if accelerator.is_local_main_process:\n        accelerator.init_trackers(\n        project_name=\"distributed_training\",\n        init_kwargs={\"wandb\": {\"entity\": \"ashton_h\", \"name\": \"armor-1-epoch\"}}\n        )\n    \n    train_dataloader, eval_dataloader = create_dataloaders(dataset_dict, data_collator, batch_size)\n    \n    def print_trainable_parameters(model):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in model.named_parameters():\n            all_param += param.numel()\n            if param.requires_grad:\n                trainable_params += param.numel()\n        accelerator.print(\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n        )\n        \n    # print the number of trainable parameters\n    print_trainable_parameters(model)\n\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    lr_scheduler = get_scheduler(\n        \"cosine\", optimizer=optimizer, num_warmup_steps=0.03, num_training_steps=len(train_dataloader) * num_epoch\n    )\n    \n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    \n    for epoch in range(num_epoch):\n        train_one_epoch(model, train_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n        try:\n            eval_one_epoch(model, eval_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n        except Exception as e:\n            print(e)\n            pass\n        \n    if accelerator.is_local_main_process:\n        accelerator.end_training()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:23:13.534426Z","iopub.execute_input":"2024-07-19T17:23:13.535171Z","iopub.status.idle":"2024-07-19T17:23:13.557269Z","shell.execute_reply.started":"2024-07-19T17:23:13.535125Z","shell.execute_reply":"2024-07-19T17:23:13.556410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = RewardDataCollatorWithPadding(tokenizer, padding='longest')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:23:17.437663Z","iopub.execute_input":"2024-07-19T17:23:17.438419Z","iopub.status.idle":"2024-07-19T17:23:17.442496Z","shell.execute_reply.started":"2024-07-19T17:23:17.438377Z","shell.execute_reply":"2024-07-19T17:23:17.441735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.init()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:25:12.935450Z","iopub.execute_input":"2024-07-19T17:25:12.935789Z","iopub.status.idle":"2024-07-19T17:25:48.334935Z","shell.execute_reply.started":"2024-07-19T17:25:12.935759Z","shell.execute_reply":"2024-07-19T17:25:48.334215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import notebook_launcher\nargs = (dataset_dict, data_collator, \"bf16\", BATCH_SIZE, GRADIENT_ACCUMULATION_STEPS, NUM_EPOCH, LEARNING_RATE)\nnotebook_launcher(main_train_loop, args, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T17:25:53.591364Z","iopub.execute_input":"2024-07-19T17:25:53.591798Z","iopub.status.idle":"2024-07-19T17:26:05.814375Z","shell.execute_reply.started":"2024-07-19T17:25:53.591749Z","shell.execute_reply":"2024-07-19T17:26:05.813103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}