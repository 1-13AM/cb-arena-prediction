{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T06:48:24.657311Z","iopub.status.busy":"2024-07-30T06:48:24.656217Z","iopub.status.idle":"2024-07-30T06:51:10.171794Z","shell.execute_reply":"2024-07-30T06:51:10.170809Z","shell.execute_reply.started":"2024-07-30T06:48:24.657265Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 24.4.1 requires cubinlinker, which is not installed.\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n","cudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install datasets bitsandbytes accelerate torch peft wandb -U -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T06:51:10.175005Z","iopub.status.busy":"2024-07-30T06:51:10.174150Z","iopub.status.idle":"2024-07-30T06:51:31.611148Z","shell.execute_reply":"2024-07-30T06:51:31.610223Z","shell.execute_reply.started":"2024-07-30T06:51:10.174966Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1Q_uRzfPT-Ikoxm3hCQ0Un9cbJHlEcszw\n","From (redirected): https://drive.google.com/uc?id=1Q_uRzfPT-Ikoxm3hCQ0Un9cbJHlEcszw&confirm=t&uuid=e8e7b315-4941-4798-815b-362d998155e0\n","To: /kaggle/working/preprocessed_data_v2.pkl\n","100%|█████████████████████████████████████████| 309M/309M [00:03<00:00, 101MB/s]\n"]}],"source":["!pip install gdown -U -qq\n","!gdown https://drive.google.com/uc?id=1Q_uRzfPT-Ikoxm3hCQ0Un9cbJHlEcszw"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T06:51:31.613040Z","iopub.status.busy":"2024-07-30T06:51:31.612693Z","iopub.status.idle":"2024-07-30T06:51:34.087515Z","shell.execute_reply":"2024-07-30T06:51:34.086561Z","shell.execute_reply.started":"2024-07-30T06:51:31.613004Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","import pandas as pd\n","import ast\n","import re\n","from typing import List\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n","#     torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n","#     torch.backends.cudnn.benchmark = False  # Ensure reproducibility, might slow down performance\n","    random.seed(seed)\n","\n","# Set the seed for all libraries\n","set_seed(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T06:51:34.090295Z","iopub.status.busy":"2024-07-30T06:51:34.089862Z","iopub.status.idle":"2024-07-30T06:51:34.106048Z","shell.execute_reply":"2024-07-30T06:51:34.105176Z","shell.execute_reply.started":"2024-07-30T06:51:34.090259Z"},"trusted":true},"outputs":[],"source":["import shutil\n","src_paths = [r\"/kaggle/input/armorm-llama3-8b-v0-1-mdl-custom/modeling_custom.py\", r\"/kaggle/input/lmsys-ddp-for-armor-llama/lmsys_ddp.py\"]\n","dst_path = r\"/kaggle/working/\"\n","for src_path in src_paths:\n","    shutil.copy(src_path, dst_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T06:51:34.107862Z","iopub.status.busy":"2024-07-30T06:51:34.107584Z","iopub.status.idle":"2024-07-30T06:51:34.113182Z","shell.execute_reply":"2024-07-30T06:51:34.112237Z","shell.execute_reply.started":"2024-07-30T06:51:34.107838Z"},"trusted":true},"outputs":[],"source":["INPUT_SIZE = 19\n","HIDDEN_SIZE = 128\n","NUM_CLASSES = 3\n","BATCH_SIZE = 2\n","PER_DEVICE_TRAIN_BATCH_SIZE = PER_DEVICE_EVAL_BATCH_SIZE = 1\n","NUM_EPOCH = 1\n","GRADIENT_ACCUMULATION_STEPS = 8\n","MODEL_CKPT = \"RLHFlow/ArmoRM-Llama3-8B-v0.1\"\n","LEARNING_RATE = 1e-4\n","MAX_LENGTH = 1280\n","\n","# Might take a look at LoRA to remind myself of how these hyperparameters work\n","LORA_R = 4\n","LORA_ALPHA = LORA_R * 2\n","LORA_DROPOUT = 0.05\n","LORA_BIAS = 'none'"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:26.971145Z","iopub.status.busy":"2024-07-30T07:21:26.970733Z","iopub.status.idle":"2024-07-30T07:21:31.907117Z","shell.execute_reply":"2024-07-30T07:21:31.906280Z","shell.execute_reply.started":"2024-07-30T07:21:26.971112Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_pickle('/kaggle/working/preprocessed_data_v2.pkl')\n","train_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:31.909112Z","iopub.status.busy":"2024-07-30T07:21:31.908815Z","iopub.status.idle":"2024-07-30T07:21:36.145780Z","shell.execute_reply":"2024-07-30T07:21:36.144470Z","shell.execute_reply.started":"2024-07-30T07:21:31.909087Z"},"trusted":true},"outputs":[],"source":["def truncate(df, max_length):\n","    def truncation_op(x, col: str, max_len: int = 1024):\n","        return x[col][:min(len(x[col]), max_len)]\n","    \n","    df['input_ids_a'] = df.apply(truncation_op, args=('input_ids_a', max_length), axis=1)\n","    df['input_ids_b'] = df.apply(truncation_op, args=('input_ids_b', max_length), axis=1)\n","    \n","    return df\n","\n","train_df = truncate(train_df, MAX_LENGTH)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:36.147646Z","iopub.status.busy":"2024-07-30T07:21:36.147251Z","iopub.status.idle":"2024-07-30T07:21:36.154138Z","shell.execute_reply":"2024-07-30T07:21:36.153052Z","shell.execute_reply.started":"2024-07-30T07:21:36.147613Z"},"trusted":true},"outputs":[],"source":["token_pattern = [128009, 128006, 78191, 128007, 271]\n","def find_token_for_gating(lst, ):\n","    \"\"\"Find the last occurrence of a token_pattern in a list.\"\"\"\n","    token_pattern_len = len(token_pattern)\n","    search_end = len(lst)\n","    for j in range(search_end - token_pattern_len, -1, -1):\n","        if lst[j:j + token_pattern_len] == token_pattern:\n","            return j\n","    return -1"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:36.156636Z","iopub.status.busy":"2024-07-30T07:21:36.156257Z","iopub.status.idle":"2024-07-30T07:21:43.603504Z","shell.execute_reply":"2024-07-30T07:21:43.602502Z","shell.execute_reply.started":"2024-07-30T07:21:36.156605Z"},"trusted":true},"outputs":[],"source":["# remove all data points where the prompt token lengths exceed \n","exp = []\n","for i in range(len(train_df)): \n","    if(find_token_for_gating(train_df.iloc[i]['input_ids_a']) < 0):\n","        exp.append(i)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:43.605163Z","iopub.status.busy":"2024-07-30T07:21:43.604811Z","iopub.status.idle":"2024-07-30T07:21:43.630550Z","shell.execute_reply":"2024-07-30T07:21:43.629757Z","shell.execute_reply.started":"2024-07-30T07:21:43.605131Z"},"trusted":true},"outputs":[],"source":["# remove these rows from training data        \n","train_df = train_df.drop(index=exp)\n","train_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:43.633392Z","iopub.status.busy":"2024-07-30T07:21:43.632720Z","iopub.status.idle":"2024-07-30T07:21:44.296345Z","shell.execute_reply":"2024-07-30T07:21:44.295359Z","shell.execute_reply.started":"2024-07-30T07:21:43.633360Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n","from functools import partial\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare datasets"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:44.297785Z","iopub.status.busy":"2024-07-30T07:21:44.297493Z","iopub.status.idle":"2024-07-30T07:21:51.564847Z","shell.execute_reply":"2024-07-30T07:21:51.563984Z","shell.execute_reply.started":"2024-07-30T07:21:44.297760Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","train_df_cop = train_df.copy()\n","dataset = Dataset.from_pandas(train_df_cop[['input_ids_a', 'input_ids_b', 'label']])\n","dataset.set_format(type=\"torch\")\n","shuffled_dataset = dataset.shuffle(seed=42)\n","train_test_split = shuffled_dataset.train_test_split(test_size=0.2)\n","\n","# Optionally, you can wrap them in a DatasetDict for easier handling\n","dataset_dict = DatasetDict({\n","    'train': train_test_split['train'],\n","    'validation': train_test_split['test']\n","})"]},{"cell_type":"markdown","metadata":{},"source":["# Build a network for model comparison"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.567055Z","iopub.status.busy":"2024-07-30T07:21:51.566762Z","iopub.status.idle":"2024-07-30T07:21:51.578580Z","shell.execute_reply":"2024-07-30T07:21:51.577616Z","shell.execute_reply.started":"2024-07-30T07:21:51.567029Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from modeling_custom import LlamaForRewardModelWithGating\n","\n","class SwiGLU(nn.Module):\n","    def forward(self, x):\n","        x, gate = x.chunk(2, dim=-1)\n","        return F.silu(gate) * x\n","\n","class DualInputInteractionNetwork(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(DualInputInteractionNetwork, self).__init__()\n","        self.classification_head = nn.Sequential(nn.Linear(input_size, hidden_size),\n","                                                 nn.Linear(hidden_size, hidden_size),\n","                                                 nn.Linear(hidden_size, num_classes)  # Combining interaction features\n","                                                )\n","\n","    def forward_one(self, x):\n","        x = F.silu(self.shared_fc1(x))\n","        x = F.silu(self.shared_fc2(x))\n","        return x\n","    \n","    @staticmethod\n","    def SwiGLU(x):\n","        x, gate = x.chunk(2, dim=-1)\n","        return F.silu(gate) * x\n","    \n","    def forward(self, inputs: dict, reward_transform_matrix: torch.Tensor):\n","        \n","        batch_size = inputs.rewards.shape[0]\n","        model_a_idx = torch.arange(0, batch_size, 2)\n","        model_b_idx = model_a_idx + 1\n","        \n","        multi_obj_rewards_a = inputs.rewards[model_a_idx]\n","        multi_obj_rewards_b = inputs.rewards[model_b_idx]\n","        \n","        multi_obj_coeffs_a = inputs.gating_output[model_a_idx] @ reward_transform_matrix.T\n","        multi_obj_coeffs_b = inputs.gating_output[model_b_idx] @ reward_transform_matrix.T\n","        \n","        scaled_mul_obj_rewards_a = multi_obj_rewards_a * multi_obj_coeffs_a\n","        scaled_mul_obj_rewards_b = multi_obj_rewards_b * multi_obj_coeffs_b\n","        \n","        output = self.classification_head(scaled_mul_obj_rewards_a - scaled_mul_obj_rewards_b)\n","        \n","        return output"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.580486Z","iopub.status.busy":"2024-07-30T07:21:51.579947Z","iopub.status.idle":"2024-07-30T07:21:51.596379Z","shell.execute_reply":"2024-07-30T07:21:51.595413Z","shell.execute_reply.started":"2024-07-30T07:21:51.580452Z"},"trusted":true},"outputs":[],"source":["from typing import Optional\n","\n","class LLaMaPreferencePredictionModel(nn.Module):\n","    \n","    def __init__(self, reward_model_ckpt, reward_model_quant_config: BitsAndBytesConfig, \n","                 lora_config: Optional[dict] = None, \n","                 **dual_input_interaction_kwargs):\n","        \"\"\"\n","        there are Gemma variants for ArMor\n","        \"\"\"\n","        \n","        super().__init__()\n","        self.reward_model = LlamaForRewardModelWithGating.from_pretrained(reward_model_ckpt, \n","                                                                          quantization_config=reward_model_quant_config\n","                                                                                            )\n","        \n","        self.preference_prediction_model = DualInputInteractionNetwork(**dual_input_interaction_kwargs)\n","        \n","        self.register_buffer('reward_transform_matrix', self.reward_model.reward_transform_matrix)\n","        \n","        if lora_config is not None:\n","            self.lora_setup(**lora_config)\n","            \n","    def lora_config(self, **lora_config):\n","        \n","        from peft import LoraConfig, TaskType\n","        self.lora_config = LoraConfig(**lora_config,\n","                                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n","                                task_type=TaskType.SEQ_CLS)\n","        \n","    def lora_setup(self, **lora_config):\n","        \n","        from peft import get_peft_model, prepare_model_for_kbit_training\n","        self.lora_config(**lora_config)\n","        \n","        self.reward_model = prepare_model_for_kbit_training(self.reward_model)\n","        self.reward_model = get_peft_model(self.reward_model, self.lora_config)\n","    \n","        \n","    def forward(self, input_ids, attention_mask, label=None):\n","        # there are data fields that the ArMor model does not require, so passing kwargs would be the temporary solution\n","        # armoRM framework only takes input_ids as inputs\n","        \n","        reward_model_output = self.reward_model(input_ids, attention_mask)\n","        logits = self.preference_prediction_model(reward_model_output, self.reward_transform_matrix)\n","        if label is None:\n","            return {\"reward_model_output\": reward_model_output,\n","                    \"logits\": logits}\n","        \n","        return {\"reward_model_output\": reward_model_output,\n","                \"logits\": logits,\n","                \"label\": label}"]},{"cell_type":"markdown","metadata":{},"source":["### Create a custom data collator & trainer for this use case"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.598305Z","iopub.status.busy":"2024-07-30T07:21:51.597802Z","iopub.status.idle":"2024-07-30T07:21:51.612322Z","shell.execute_reply":"2024-07-30T07:21:51.611476Z","shell.execute_reply.started":"2024-07-30T07:21:51.598274Z"},"trusted":true},"outputs":[],"source":["\n","from transformers import Trainer\n","from transformers import DataCollatorWithPadding\n","from dataclasses import dataclass\n","from typing import Union, Dict, Any\n","from transformers.tokenization_utils_base import PaddingStrategy\n","\n","@dataclass\n","class RewardDataCollatorWithPadding:\n","    tokenizer: AutoTokenizer\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n","        merged_features = []\n","        for feature in features:\n","            merged_features.append(\n","                {\n","                    \"input_ids\": feature[\"input_ids_a\"]\n","                }\n","            )\n","            merged_features.append(\n","                {\n","                    \"input_ids\": feature[\"input_ids_b\"]\n","                }\n","            )\n","        batch = self.tokenizer.pad(\n","            merged_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=self.return_tensors,\n","        )\n","        batch = {\n","            \"input_ids\": batch[\"input_ids\"],\n","            \"attention_mask\": batch[\"attention_mask\"],\n","            \"label\": torch.tensor([feature[\"label\"].item() for feature in features])\n","        }\n","        \n","        return batch\n","                    \n","class RewardTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        outputs = model(\n","            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], label=inputs['label']\n","        )\n","        logits, targets = outputs['logits'], outputs['label']\n","        loss = nn.functional.cross_entropy(logits, targets).mean()\n","        \n","        return loss\n","\n","def compute_metrics(pred):\n","    \n","    # Get the predictions and labels from the pred argument\n","    logits, labels = pred\n","    predictions = np.argmax(logits, axis=-1)\n","    \n","    # Calculate cross entropy loss\n","    # Convert logits to PyTorch tensor\n","    logits_tensor = torch.tensor(logits)\n","    labels_tensor = torch.tensor(labels)\n","    \n","    # Compute cross entropy loss\n","    cross_entropy_loss = F.cross_entropy(logits_tensor, labels_tensor).item()\n","\n","    return {\n","        \"cross_entropy_loss\": cross_entropy_loss\n","    }"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.613893Z","iopub.status.busy":"2024-07-30T07:21:51.613561Z","iopub.status.idle":"2024-07-30T07:21:51.629496Z","shell.execute_reply":"2024-07-30T07:21:51.628592Z","shell.execute_reply.started":"2024-07-30T07:21:51.613864Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n","from modeling_custom import LlamaForRewardModelWithGating\n","import torch\n","\n","nf4_config = BitsAndBytesConfig(\n","   load_in_4bit=True,\n","   bnb_4bit_quant_type=\"nf4\",\n","   bnb_4bit_use_double_quant=True, # quantize the quantization factor, which saves another 0.4 bit/parameter\n","   bnb_4bit_compute_dtype=torch.float16 # configure computations to be in (b)float16\n",")\n","\n","lora_config = {'r': LORA_R,\n","               'lora_alpha': LORA_ALPHA,\n","               'lora_dropout': LORA_DROPOUT,\n","               'bias': LORA_BIAS\n","                }"]},{"cell_type":"markdown","metadata":{},"source":["### Multi-GPU training"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.631262Z","iopub.status.busy":"2024-07-30T07:21:51.630943Z","iopub.status.idle":"2024-07-30T07:21:51.661378Z","shell.execute_reply":"2024-07-30T07:21:51.660630Z","shell.execute_reply.started":"2024-07-30T07:21:51.631239Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, TensorDataset, DistributedSampler\n","from tqdm import tqdm\n","from accelerate import Accelerator\n","from torch.optim import AdamW\n","from transformers import get_scheduler\n","\n","def create_dataloaders(dataset_dict: DatasetDict, data_collator, batch_size: int = 4):\n","    train_dataloader = DataLoader(dataset_dict['train'], batch_size=batch_size, collate_fn=data_collator)\n","    eval_dataloader = DataLoader(dataset_dict['validation'], batch_size=batch_size, collate_fn=data_collator)\n","    return train_dataloader, eval_dataloader\n","\n","def train_one_epoch(model, dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch):\n","    accelerator.print(\"Training on one epoch\")\n","    model.train()\n","    epoch_loss, step_loss = 0., 0.\n","    for i, batch in enumerate(tqdm(dataloader, desc=f'Epoch {epoch+1}')):\n","        print(batch['input_ids'].shape)\n","        with accelerator.accumulate(model):\n","            # Forward pass\n","            outputs = model(\n","                input_ids=batch['input_ids'],\n","                attention_mask=batch[\"attention_mask\"], \n","                label=batch['label']\n","            )\n","            \n","            logits = outputs['logits']\n","            loss = criterion(logits, batch['label'])\n","            step_loss += loss.item()\n","            epoch_loss += loss.item()\n","            \n","            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n","                accelerator.log({\"train_loss\": step_loss / GRADIENT_ACCUMULATION_STEPS, \"epoch\": epoch + 1})\n","                step_loss = 0.\n","                \n","            accelerator.backward(loss)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","    accelerator.print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}\")\n","    \n","def eval_one_epoch(model, dataloader, criterion, accelerator, epoch):\n","    model.eval()\n","    epoch_loss = 0\n","    \n","    for batch in tqdm(dataloader):\n","\n","        # Forward pass\n","        outputs = model(\n","                input_ids=batch['input_ids'],\n","                attention_mask=batch[\"attention_mask\"], \n","                label=batch['label']\n","            )\n","        \n","        logits = outputs['logits']\n","        loss = criterion(logits, batch['label'])\n","        epoch_loss += loss.item()\n","    \n","    accelerator.log({\"epoch_loss\": epoch_loss / len(dataloader), \"epoch\": epoch + 1})\n","    \n","    accelerator.print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}\")\n","    \n","\n","def main_train_loop(dataset_dict, data_collator, mixed_precision, batch_size, gradient_accumulation_steps, num_epoch, learning_rate):\n","    \n","    accelerator = Accelerator(mixed_precision=mixed_precision, \n","                              gradient_accumulation_steps=gradient_accumulation_steps,\n","                              log_with=\"wandb\"\n","                             )\n","    \n","    model = LLaMaPreferencePredictionModel(reward_model_ckpt=MODEL_CKPT, \n","                                       reward_model_quant_config=nf4_config, \n","                                       lora_config=lora_config,\n","                                       input_size=INPUT_SIZE, \n","                                       hidden_size=HIDDEN_SIZE, \n","                                       num_classes=NUM_CLASSES) \n","    \n","    if accelerator.is_local_main_process:\n","        accelerator.init_trackers(\n","        project_name=\"distributed_training\",\n","        init_kwargs={\"wandb\": {\"entity\": \"ashton_h\", \"name\": \"armor-1-epoch\"}}\n","        )\n","    \n","    train_dataloader, eval_dataloader = create_dataloaders(dataset_dict, data_collator, batch_size)\n","    \n","    def print_trainable_parameters(model):\n","        \"\"\"\n","        Prints the number of trainable parameters in the model.\n","        \"\"\"\n","        trainable_params = 0\n","        all_param = 0\n","        for _, param in model.named_parameters():\n","            all_param += param.numel()\n","            if param.requires_grad:\n","                trainable_params += param.numel()\n","        accelerator.print(\n","            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","        )\n","        \n","    # print the number of trainable parameters\n","    print_trainable_parameters(model)\n","\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","    lr_scheduler = get_scheduler(\n","        \"cosine\", optimizer=optimizer, num_warmup_steps=0.02, num_training_steps=len(train_dataloader) * num_epoch\n","    )\n","    \n","    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n","    \n","    for epoch in range(num_epoch):\n","        train_one_epoch(model, train_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n","#         try:\n","#             eval_one_epoch(model, eval_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n","#         except Exception as e:\n","#             print(e)\n","#             pass\n","        \n","    if accelerator.is_local_main_process:\n","        accelerator.end_training()"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:21:51.664054Z","iopub.status.busy":"2024-07-30T07:21:51.663772Z","iopub.status.idle":"2024-07-30T07:21:51.674335Z","shell.execute_reply":"2024-07-30T07:21:51.673628Z","shell.execute_reply.started":"2024-07-30T07:21:51.664027Z"},"trusted":true},"outputs":[],"source":["data_collator = RewardDataCollatorWithPadding(tokenizer, padding='longest')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:24:40.285267Z","iopub.status.busy":"2024-07-30T07:24:40.284349Z","iopub.status.idle":"2024-07-30T07:24:40.288898Z","shell.execute_reply":"2024-07-30T07:24:40.287983Z","shell.execute_reply.started":"2024-07-30T07:24:40.285236Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.init()"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-07-30T07:24:41.207931Z","iopub.status.busy":"2024-07-30T07:24:41.207116Z","iopub.status.idle":"2024-07-30T07:36:24.920980Z","shell.execute_reply":"2024-07-30T07:36:24.919430Z","shell.execute_reply.started":"2024-07-30T07:24:41.207900Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching training on 2 GPUs.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ebfa4172361403f9aebdd807d3f29ed","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b816c46c23f4a6c90dfc16506c8a9f7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["trainable params: 2378755 || all params: 4020854144 || trainable%: 0.05916043991671835\n","Training on one epoch\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 0/11409 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Epoch 1:   0%|          | 0/11409 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 1055])\n","torch.Size([4, 787])\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","Epoch 1:   0%|          | 1/11409 [00:59<190:01:07, 59.96s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 437])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 1/11409 [01:16<241:11:43, 76.11s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 187])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 2/11409 [01:37<138:28:56, 43.70s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 642])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 2/11409 [01:41<156:05:50, 49.26s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 491])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 3/11409 [02:25<145:46:45, 46.01s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 305])\n","torch.Size([4, 388])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 4/11409 [02:52<123:52:54, 39.10s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 547])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 4/11409 [02:58<128:55:17, 40.69s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 584])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 5/11409 [03:39<133:15:22, 42.07s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 892])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 5/11409 [03:43<134:06:18, 42.33s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 498])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 6/11409 [04:24<132:19:37, 41.78s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 1280])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 6/11409 [04:55<169:22:12, 53.47s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 325])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 7/11409 [05:27<147:23:39, 46.54s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 750])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 7/11409 [06:03<191:42:53, 60.53s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 1280])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 8/11409 [07:54<248:16:27, 78.40s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 766])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 8/11409 [07:54<242:20:15, 76.52s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 275])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 9/11409 [08:15<187:34:41, 59.24s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 390])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 10/11409 [08:46<160:11:01, 50.59s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 808])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 9/11409 [08:51<227:41:16, 71.90s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 797])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 11/11409 [09:49<171:45:10, 54.25s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 306])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 10/11409 [09:58<223:02:21, 70.44s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 1280])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 12/11409 [10:17<146:36:15, 46.31s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([4, 596])\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 10/11409 [10:38<202:03:28, 63.81s/it]\n","W0730 07:35:58.770000 140582112737088 torch/multiprocessing/spawn.py:146] Terminating process 3645 via signal SIGTERM\n","Epoch 1:   0%|          | 12/11409 [11:04<175:13:35, 55.35s/it]\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] failed (exitcode: 1) local_rank: 1 (pid: 3648) of fn: main_train_loop (start_method: fork)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 659, in _poll\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     self._pc.join(-1)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 189, in join\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] torch.multiprocessing.spawn.ProcessRaisedException: \n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] \n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] -- Process 1 terminated with the following error:\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     fn(i, *args)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 583, in _wrap\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     ret = record(fn)(*args_)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return f(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/tmp/ipykernel_34/1829121754.py\", line 113, in main_train_loop\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     train_one_epoch(model, train_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/tmp/ipykernel_34/1829121754.py\", line 20, in train_one_epoch\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     outputs = model(\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     else self._run_ddp_forward(*inputs, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self.module(*inputs, **kwargs)  # type: ignore[index]\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 819, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return model_forward(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 807, in __call__\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return func(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/tmp/ipykernel_34/4154969323.py\", line 44, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     reward_model_output = self.reward_model(input_ids, attention_mask)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1379, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self.base_model(\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     return self.model.forward(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 169, in new_forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     output = module._old_forward(*args, **kwargs)\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/kaggle/working/modeling_custom.py\", line 152, in forward\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/kaggle/working/modeling_custom.py\", line 152, in <listcomp>\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/kaggle/working/modeling_custom.py\", line 47, in find_token_for_gating\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702]     raise ValueError(\"Token pattern not found in the list.\")\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] ValueError: Token pattern not found in the list.\n","E0730 07:36:24.790000 140582112737088 torch/distributed/elastic/multiprocessing/api.py:702] \n"]},{"ename":"ChildFailedError","evalue":"\n============================================================\nmain_train_loop FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-30_07:35:58\n  host      : b66de24995fc\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 3648)\n  error_file: /tmp/torchelastic_etfd9ghy/none_q0ql9aa8/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_34/1829121754.py\", line 113, in main_train_loop\n      train_one_epoch(model, train_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n    File \"/tmp/ipykernel_34/1829121754.py\", line 20, in train_one_epoch\n      outputs = model(\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n      else self._run_ddp_forward(*inputs, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n      return self.module(*inputs, **kwargs)  # type: ignore[index]\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 819, in forward\n      return model_forward(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 807, in __call__\n      return convert_to_fp32(self.model_forward(*args, **kwargs))\n    File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n      return func(*args, **kwargs)\n    File \"/tmp/ipykernel_34/4154969323.py\", line 44, in forward\n      reward_model_output = self.reward_model(input_ids, attention_mask)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1379, in forward\n      return self.base_model(\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n      return self.model.forward(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 169, in new_forward\n      output = module._old_forward(*args, **kwargs)\n    File \"/kaggle/working/modeling_custom.py\", line 152, in forward\n      gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n    File \"/kaggle/working/modeling_custom.py\", line 152, in <listcomp>\n      gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n    File \"/kaggle/working/modeling_custom.py\", line 47, in find_token_for_gating\n      raise ValueError(\"Token pattern not found in the list.\")\n  ValueError: Token pattern not found in the list.\n  \n============================================================","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m (dataset_dict, data_collator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m, BATCH_SIZE, GRADIENT_ACCUMULATION_STEPS, NUM_EPOCH, LEARNING_RATE)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_train_loop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/launchers.py:245\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    244\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 245\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py:133\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py:264\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    265\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    266\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n","\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\nmain_train_loop FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-30_07:35:58\n  host      : b66de24995fc\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 3648)\n  error_file: /tmp/torchelastic_etfd9ghy/none_q0ql9aa8/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_34/1829121754.py\", line 113, in main_train_loop\n      train_one_epoch(model, train_dataloader, optimizer, criterion, accelerator, lr_scheduler, epoch)\n    File \"/tmp/ipykernel_34/1829121754.py\", line 20, in train_one_epoch\n      outputs = model(\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n      else self._run_ddp_forward(*inputs, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n      return self.module(*inputs, **kwargs)  # type: ignore[index]\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 819, in forward\n      return model_forward(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 807, in __call__\n      return convert_to_fp32(self.model_forward(*args, **kwargs))\n    File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n      return func(*args, **kwargs)\n    File \"/tmp/ipykernel_34/4154969323.py\", line 44, in forward\n      reward_model_output = self.reward_model(input_ids, attention_mask)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1379, in forward\n      return self.base_model(\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n      return self.model.forward(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 169, in new_forward\n      output = module._old_forward(*args, **kwargs)\n    File \"/kaggle/working/modeling_custom.py\", line 152, in forward\n      gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n    File \"/kaggle/working/modeling_custom.py\", line 152, in <listcomp>\n      gating_token_positions = [find_token_for_gating(ids.tolist()) for ids in input_ids]\n    File \"/kaggle/working/modeling_custom.py\", line 47, in find_token_for_gating\n      raise ValueError(\"Token pattern not found in the list.\")\n  ValueError: Token pattern not found in the list.\n  \n============================================================"]}],"source":["from accelerate import notebook_launcher\n","args = (dataset_dict, data_collator, \"bf16\", BATCH_SIZE, GRADIENT_ACCUMULATION_STEPS, NUM_EPOCH, LEARNING_RATE)\n","notebook_launcher(main_train_loop, args, num_processes=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:53:18.919506Z","iopub.status.idle":"2024-07-30T06:53:18.920043Z","shell.execute_reply":"2024-07-30T06:53:18.919793Z","shell.execute_reply.started":"2024-07-30T06:53:18.919766Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:53:18.921323Z","iopub.status.idle":"2024-07-30T06:53:18.921845Z","shell.execute_reply":"2024-07-30T06:53:18.921597Z","shell.execute_reply.started":"2024-07-30T06:53:18.921576Z"},"trusted":true},"outputs":[],"source":["print(\"hi there\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":5350506,"sourceId":8900034,"sourceType":"datasetVersion"},{"datasetId":5361778,"sourceId":8915911,"sourceType":"datasetVersion"},{"datasetId":5381992,"isSourceIdPinned":true,"sourceId":8944605,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
